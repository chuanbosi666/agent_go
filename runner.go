package nvgo

import (
	"context"
	"errors"
	"fmt"

	"github.com/agent_go/memory"
	"github.com/openai/openai-go/v3"
	"github.com/openai/openai-go/v3/option"
	"github.com/openai/openai-go/v3/responses"
)

type RunItem interface {
	isRunItem()
	ToInputItem() responses.ResponseInputItemUnionParam
}

type Usage struct {
	// Total requests made to the LLM API.
	Requests uint64

	// Total input tokens sent, across all requests.
	InputTokens uint64

	// Details about the input tokens, matching responses API usage details.
	InputTokensDetails responses.ResponseUsageInputTokensDetails

	// Total output tokens received, across all requests.
	OutputTokens uint64

	// Details about the output tokens, matching responses API usage details.
	OutputTokensDetails responses.ResponseUsageOutputTokensDetails

	// Total tokens sent and received, across all requests.
	TotalTokens uint64
}

type ModelResponse struct {
	// A list of outputs (messages, tool calls, etc.) generated by the model
	Output []responses.ResponseOutputItemUnion

	// The usage information for the response.
	Usage *Usage

	// Optional ID for the response which can be used to refer to the response in subsequent calls to the
	// model. Not supported by all model providers.
	// If using OpenAI models via the Responses API, this is the `ResponseID` parameter, and it can
	// be passed to `Runner.Run`.
	ResponseID string
}

type RunResult struct {
	// The original input items i.e. the items before Run() was called. This may be a mutated
	// version of the input, if there are handoff input filters that mutate the input.
	Input Input

	// The new items generated during the agent run.
	// These include things like new messages, tool calls and their outputs, etc.
	NewItems []RunItem

	// The raw LLM responses generated by the model during the agent run.
	RawResponses []ModelResponse

	// The output of the last agent.
	FinalOutput any

	// Guardrail results for the input messages.
	InputGuardrailResults []InputGuardrailResult

	// Guardrail results for the final output of the agent.
	OutputGuardrailResults []OutputGuardrailResult

	// The LastAgent that was run.
	LastAgent *Agent
}

const DefaultMaxTurns = 10

// DefaultRunner is the default Runner instance used by package-level Run
// helpers.
var DefaultRunner = Runner{}

// Runner executes agents using the configured RunConfig.
//
// The zero value is valid.
type Runner struct {
	Config RunConfig
}

const DefaultWorkflowName = "Agent workflow"

// RunConfig configures settings for the entire agent run.
type RunConfig struct {
	// The model to use for the entire agent run. If set, will override the model set on every
	// agent. The ModelProvider passed in below must be able to resolve this model name.
	Model string

	// Optional global model settings. Any non-null or non-zero values will
	// override the agent-specific model settings.
	ModelSettings ModelSettings

	// A list of input guardrails to run on the initial run input.
	InputGuardrails []InputGuardrail

	// A list of output guardrails to run on the final output of the run.
	OutputGuardrails []OutputGuardrail

	// The name of the run, used for tracing. Should be a logical name for the run, like
	// "Code generation workflow" or "Customer support agent".
	// Default: DefaultWorkflowName.
	WorkflowName string

	// Optional maximum number of turns to run the agent for.
	// A turn is defined as one AI invocation (including any tool calls that might occur).
	// Default (when left zero): DefaultMaxTurns.
	MaxTurns uint64

	// Optional ID of the previous response, if using OpenAI models via the Responses API,
	// this allows you to skip passing in input from the previous turn.
	PreviousResponseID string

	// Optional session for the run.
	Session memory.Session
}

// Run executes startingAgent with the provided input using the DefaultRunner.
func Run(ctx context.Context, startingAgent *Agent, input string) (*RunResult, error) {
	return DefaultRunner.Run(ctx, startingAgent, input)
}

// Run a workflow starting at the given agent. The agent will run in a loop until a final
// output is generated.
//
// The loop runs like so:
//  1. The agent is invoked with the given input.
//  2. If there is a final output (i.e. the agent produces something of type Agent.OutputType, the loop terminates.
//  3. If there's a handoff, we run the loop again, with the new agent.
//  4. Else, we run tool calls (if any), and re-run the loop.
//
// In two cases, the agent run may return an error:
//  1. If the MaxTurns is exceeded, a MaxTurnsExceededError is returned.
//  2. If a guardrail tripwire is triggered, a *GuardrailTripwireTriggeredError is returned.
//
// Note that only the first agent's input guardrails are run.
//
// It returns a run result containing all the inputs, guardrail results and the output of the last
// agent. Agents may perform handoffs, so we don't know the specific type of the output.
func (r Runner) Run(ctx context.Context, startingAgent *Agent, input string) (*RunResult, error) {
	return r.run(ctx, startingAgent, InputString(input))
}

type MaxTurnsExceededError struct {
	MaxTurns uint64
}

func (e *MaxTurnsExceededError) Error() string {
	return fmt.Sprintf("max turns exceeded: reached limit of %d turns", e.MaxTurns)
}

type GuardrailTripwireTriggeredError struct {
	GuardrailName string // 哪个护栏触发了
	OutputInfo    any    // 护栏返回的详细信息
	IsInput       bool   // true = 输入护栏, false = 输出护栏
}

func (g *GuardrailTripwireTriggeredError) Error() string {
	if g.IsInput {
		return fmt.Sprintf("input guardrail '%s' triggered", g.GuardrailName)
	}
	return fmt.Sprintf("output guardrail '%s' triggered", g.GuardrailName)
}

func (r Runner) run(ctx context.Context, startingAgent *Agent, input Input) (*RunResult, error) {
	return nil, nil
}
