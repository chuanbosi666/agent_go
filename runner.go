package nvgo

import (
	"context"
	// "errors"  // 阶段 5 (Guardrails) 会使用
	"fmt"

	"github.com/agent_go/memory"
	"github.com/openai/openai-go/v3"
	_ "github.com/openai/openai-go/v3"        // 阶段 3 后期实现真正的 LLM 调用时会使用
	_ "github.com/openai/openai-go/v3/option" // 阶段 3 后期实现真正的 LLM 调用时会使用
	"github.com/openai/openai-go/v3/packages/param"
	"github.com/openai/openai-go/v3/responses"
)

type RunItem interface {
	isRunItem()
	ToInputItem() responses.ResponseInputItemUnionParam
}

type RunItemWrapper struct {
	item responses.ResponseInputItemUnionParam
}

func (w RunItemWrapper) isRunItem() {}
func (w RunItemWrapper) ToInputItem() responses.ResponseInputItemUnionParam {
	return w.item
}

func WrapRunItem(item responses.ResponseInputItemUnionParam) RunItem {
	return RunItemWrapper{item: item}
}

type Usage struct {
	// Total requests made to the LLM API.
	Requests uint64

	// Total input tokens sent, across all requests.
	InputTokens uint64

	// Details about the input tokens, matching responses API usage details.
	InputTokensDetails responses.ResponseUsageInputTokensDetails

	// Total output tokens received, across all requests.
	OutputTokens uint64

	// Details about the output tokens, matching responses API usage details.
	OutputTokensDetails responses.ResponseUsageOutputTokensDetails

	// Total tokens sent and received, across all requests.
	TotalTokens uint64
}

type ModelResponse struct {
	// A list of outputs (messages, tool calls, etc.) generated by the model
	Output []responses.ResponseOutputItemUnion

	// The usage information for the response.
	Usage *Usage

	// Optional ID for the response which can be used to refer to the response in subsequent calls to the
	// model. Not supported by all model providers.
	// If using OpenAI models via the Responses API, this is the `ResponseID` parameter, and it can
	// be passed to `Runner.Run`.
	ResponseID string
}

type RunResult struct {
	// The original input items i.e. the items before Run() was called. This may be a mutated
	// version of the input, if there are handoff input filters that mutate the input.
	Input Input

	// The new items generated during the agent run.
	// These include things like new messages, tool calls and their outputs, etc.
	NewItems []RunItem

	// The raw LLM responses generated by the model during the agent run.
	RawResponses []ModelResponse

	// The output of the last agent.
	FinalOutput any

	// Guardrail results for the input messages.
	InputGuardrailResults []InputGuardrailResult

	// Guardrail results for the final output of the agent.
	OutputGuardrailResults []OutputGuardrailResult

	// The LastAgent that was run.
	LastAgent *Agent
}

const DefaultMaxTurns = 10

// DefaultRunner is the default Runner instance used by package-level Run
// helpers.
var DefaultRunner = Runner{}

// Runner executes agents using the configured RunConfig.
//
// The zero value is valid.
type Runner struct {
	Config RunConfig
}

const DefaultWorkflowName = "Agent workflow"

// RunConfig configures settings for the entire agent run.
type RunConfig struct {
	// The model to use for the entire agent run. If set, will override the model set on every
	// agent. The ModelProvider passed in below must be able to resolve this model name.
	Model string

	// Optional global model settings. Any non-null or non-zero values will
	// override the agent-specific model settings.
	ModelSettings ModelSettings

	// A list of input guardrails to run on the initial run input.
	InputGuardrails []InputGuardrail

	// A list of output guardrails to run on the final output of the run.
	OutputGuardrails []OutputGuardrail

	// The name of the run, used for tracing. Should be a logical name for the run, like
	// "Code generation workflow" or "Customer support agent".
	// Default: DefaultWorkflowName.
	WorkflowName string

	// Optional maximum number of turns to run the agent for.
	// A turn is defined as one AI invocation (including any tool calls that might occur).
	// Default (when left zero): DefaultMaxTurns.
	MaxTurns uint64

	// Optional ID of the previous response, if using OpenAI models via the Responses API,
	// this allows you to skip passing in input from the previous turn.
	PreviousResponseID string

	// Optional session for the run.
	Session memory.Session
}

// Run executes startingAgent with the provided input using the DefaultRunner.
func Run(ctx context.Context, startingAgent *Agent, input string) (*RunResult, error) {
	return DefaultRunner.Run(ctx, startingAgent, input)
}

// Run a workflow starting at the given agent. The agent will run in a loop until a final
// output is generated.
//
// The loop runs like so:
//  1. The agent is invoked with the given input.
//  2. If there is a final output (i.e. the agent produces something of type Agent.OutputType, the loop terminates.
//  3. If there's a handoff, we run the loop again, with the new agent.
//  4. Else, we run tool calls (if any), and re-run the loop.
//
// In two cases, the agent run may return an error:
//  1. If the MaxTurns is exceeded, a MaxTurnsExceededError is returned.
//  2. If a guardrail tripwire is triggered, a *GuardrailTripwireTriggeredError is returned.
//
// Note that only the first agent's input guardrails are run.
//
// It returns a run result containing all the inputs, guardrail results and the output of the last
// agent. Agents may perform handoffs, so we don't know the specific type of the output.
func (r Runner) Run(ctx context.Context, startingAgent *Agent, input string) (*RunResult, error) {
	return r.run(ctx, startingAgent, InputString(input))
}

type MaxTurnsExceededError struct {
	MaxTurns uint64
}

func (e *MaxTurnsExceededError) Error() string {
	return fmt.Sprintf("max turns exceeded: reached limit of %d turns", e.MaxTurns)
}

type GuardrailTripwireTriggeredError struct {
	GuardrailName string // 哪个护栏触发了
	OutputInfo    any    // 护栏返回的详细信息
	IsInput       bool   // true = 输入护栏, false = 输出护栏
}

func (g *GuardrailTripwireTriggeredError) Error() string {
	if g.IsInput {
		return fmt.Sprintf("input guardrail '%s' triggered", g.GuardrailName)
	}
	return fmt.Sprintf("output guardrail '%s' triggered", g.GuardrailName)
}

func (r Runner) run(ctx context.Context, startingAgent *Agent, input Input) (*RunResult, error) {

	result := &RunResult{
		Input:        CopyInput(input),
		NewItems:     []RunItem{},
		RawResponses: []ModelResponse{},
	}
	inputGuardrails := append(r.Config.InputGuardrails, startingAgent.InputGuardrails...)

	for _, gr := range inputGuardrails {
		grResult, err := gr.Run(ctx, startingAgent, input)
		if err != nil {
			return nil, fmt.Errorf("input guardrail %q failed: %w", gr.Name, err)
		}
		result.InputGuardrailResults = append(result.InputGuardrailResults, grResult)

		if grResult.Output.TripwireTriggered {
			return nil, &GuardrailTripwireTriggeredError{
				GuardrailName: gr.Name,
				OutputInfo:    grResult.Output.OutputInfo,
				IsInput:       true,
			}
		}
	}

	currentAgent := startingAgent
	turnCount := uint64(0)
	maxTurns := r.Config.MaxTurns
	if maxTurns == 0 {
		maxTurns = DefaultMaxTurns
	}

	for turnCount < maxTurns {
		turnCount++
		model := r.Config.Model
		if model == "" {
			model = currentAgent.Model
		}

		var instructions string

		if currentAgent.Instructions != nil {
			var err error
			instructions, err = currentAgent.Instructions.GetInstructions(ctx, currentAgent)
			if err != nil {
				return nil, fmt.Errorf("get instruction: %w", err)
			}
		}

		var tools []Tool
		tools, err := getMCPTools(ctx, currentAgent, true)
		if err != nil{
			return nil, fmt.Errorf("failed to get MCP tools: %w", err)
		}
		modelsettings := currentAgent.ModelSettings.Resolve(r.Config.ModelSettings)

		var historyItems []responses.ResponseInputItemUnionParam
		if r.Config.Session != nil {
			items, err := r.Config.Session.GetItems(ctx, -1)
			if err != nil {
				return nil, fmt.Errorf("load session history: %w", err)
			}
			historyItems = items
		}

		var modelResponse ModelResponse

		if currentAgent.Prompt != nil{
			promptParam, hasPrompt, err := PromptUtil().ToModelInput(ctx, currentAgent.Prompt, currentAgent)
			if err != nil{
				return nil, fmt.Errorf("get prompt: %w", err)
			}
			if !hasPrompt{
				return nil, fmt.Errorf("prompt is required but not provided")
			}

			var allInputItems []responses.ResponseInputItemUnionParam
			allInputItems = append(allInputItems, historyItems...)

			if turnCount == 1{
				currentInputItems := inputToItems(input)
				allInputItems = append(allInputItems, currentInputItems...)
			}

			toolParams := toolsToParams(tools)

			createParams := responses.ResponseNewParams{
				Model: model,
				Prompt: promptParam,
				Input: responses.ResponseNewParamsInputUnion{
					OfInputItemList: responses.ResponseInputParam(allInputItems),
				},
			}

			if instructions != ""{
				createParams.Instructions = param.NewOpt(instructions)
			}
			if len(toolParams) > 0{
				createParams.Tools = toolParams
			}
			if modelsettings.Temperature.Valid(){
				createParams.Temperature = modelsettings.Temperature
			}
			if modelsettings.MaxTokens.Valid(){
				createParams.MaxOutputTokens = modelsettings.MaxTokens
			}
			if modelsettings.TopP.Valid(){
				createParams.TopP = modelsettings.TopP
			}
			client := currentAgent.Client

			resp, err := client.Responses.New(ctx, createParams)
			if err != nil{
				return nil, fmt.Errorf("call repsonses API: %w", err)
			}

			modelResponse = ModelResponse{
				Output: resp.Output,
				ResponseID: resp.ID,
			}
				modelResponse.Usage = &Usage{
					Requests: 				1,
					InputTokens: 			uint64(resp.Usage.InputTokens),
					InputTokensDetails: 	resp.Usage.InputTokensDetails,
					OutputTokens: 			uint64(resp.Usage.OutputTokens),
					OutputTokensDetails: 	resp.Usage.OutputTokensDetails,
					TotalTokens: 			uint64(resp.Usage.TotalTokens),		
			}
		}else{
			var messages []openai.ChatCompletionMessageParamUnion

			if instructions != ""{
				messages = append(messages, openai.SystemMessage(instructions))
			}

			if turnCount == 1{
				switch v := input.(type){
				case InputString:
					messages = append(messages, openai.UserMessage(string(v)))
				}
			}

			chatParams := openai.ChatCompletionNewParams{
				Model: model,
				Messages: messages,
			}
			if modelsettings.Temperature.Valid(){
				chatParams.Temperature = modelsettings.Temperature
			}
			if modelsettings.MaxTokens.Valid(){
				chatParams.MaxTokens = modelsettings.MaxTokens
			}
			if modelsettings.TopP.Valid(){
				chatParams.TopP = modelsettings.TopP
			}

			client := currentAgent.Client
			chatresp, err := client.Chat.Completions.New(ctx, chatParams)
			if err != nil{
				return nil, fmt.Errorf("call chat completions API:%w", err)
			}

			modelResponse = ModelResponse{
				Output: []responses.ResponseOutputItemUnion{},
			}

			if len(chatresp.Choices)>0{
				choice := chatresp.Choices[0]
				if choice.Message.Content != ""{
					//暂时只输出文本，不转化为repsonseoutputMessage
					//直接设置finaloutput
					result.FinalOutput = choice.Message.Content
				}
			}
			modelResponse.Usage = &Usage{
				Requests: 1,
				InputTokens: uint64(chatresp.Usage.PromptTokens),
				OutputTokens: uint64(chatresp.Usage.CompletionTokens),
				TotalTokens: uint64(chatresp.Usage.TotalTokens),
			}
		}
		result.RawResponses = append(result.RawResponses, modelResponse)

		for _, outputItem := range modelResponse.Output {
			switch item := outputItem.AsAny().(type) {
			case responses.ResponseOutputMessage:
				_ = item

			case responses.ResponseFunctionToolCall:
				tool, found := findTool(tools, item.Name)
				if !found {
					errorOutput := responses.ResponseInputItemParamOfFunctionCallOutput(
						item.CallID,
						fmt.Sprintf("Tool %s not found", item.Name),
					)
					result.NewItems = append(result.NewItems, WrapRunItem(errorOutput))
					continue
				}
				toolResult, err := executeTool(ctx, currentAgent, tool, item.Arguments)
				if err != nil {
					errorOutput := responses.ResponseInputItemParamOfFunctionCallOutput(
						item.CallID,
						fmt.Sprintf("Tool execution failed: %v", err),
					)
					result.NewItems = append(result.NewItems, WrapRunItem(errorOutput))
					continue
				}

				var outputStr string
				switch v := toolResult.(type) {
				case string:
					outputStr = v
				default:
					outputStr = fmt.Sprintf("%v", v)
				}
				successOutput := responses.ResponseInputItemParamOfFunctionCallOutput(
					item.CallID,
					outputStr,
				)
				result.NewItems = append(result.NewItems, WrapRunItem(successOutput))
				//切换agent
			// TODO:添加handoff，实现智能体切换
			default:
			}
		}
		for _, outputItem := range modelResponse.Output {
			if msg, ok := outputItem.AsAny().(responses.ResponseOutputMessage); ok {
				result.FinalOutput = msg.Content
				break
			}
		}

		if r.Config.Session != nil && len(result.NewItems) > 0 {
			var itemsToSave []responses.ResponseInputItemUnionParam
			for _, item := range result.NewItems {
				itemsToSave = append(itemsToSave, item.ToInputItem())
			}

			if err := r.Config.Session.AddItems(ctx, itemsToSave); err != nil {
				return nil, fmt.Errorf("save to session: %w", err)
			}
		}
		if result.FinalOutput != nil {
			break
		}

	}

	if turnCount >= maxTurns {
		return nil, &MaxTurnsExceededError{MaxTurns: maxTurns}
	}
	if result.FinalOutput == nil {
		result.LastAgent = currentAgent
		return result, nil
	}
	outputGuardrail := append(r.Config.OutputGuardrails, currentAgent.OutputGuardrails...)
	for _, gr := range outputGuardrail {
		grResult, err := gr.Run(ctx, currentAgent, result.FinalOutput)
		if err != nil {
			return nil, fmt.Errorf("output guardrail %q failed: %w", gr.Name, err)
		}

		result.OutputGuardrailResults = append(result.OutputGuardrailResults, grResult)
		if grResult.Output.TripwireTriggered {
			return nil, &GuardrailTripwireTriggeredError{
				GuardrailName: gr.Name,
				OutputInfo:    grResult.Output.OutputInfo,
				IsInput:       false,
			}
		}
	}

	result.LastAgent = currentAgent

	return result, nil
}

func getMCPTools(ctx context.Context, agent *Agent, strict bool) ([]Tool, error) {
	if len(agent.MCPServers) == 0 {
		return nil, nil
	}
	return GetAllFunctionTools(ctx, agent.MCPServers, strict, agent)
}
func findTool(tools []Tool, name string) (Tool, bool) {
	for _, t := range tools {
		if t.ToolName() == name {
			return t, true
		}
	}
	return nil, false
}

func executeTool(ctx context.Context, agent *Agent, tool Tool, arguments string) (any, error) {
	funcTool, ok := tool.(FunctionTool)
	if !ok {
		return nil, fmt.Errorf("tool is not a FunctionTool")
	}
	if funcTool.IsEnabled != nil {
		enabled, err := funcTool.IsEnabled.IsEnabled(ctx, agent)
		if err != nil {
			return nil, fmt.Errorf("check tool enabled:%w", err)
		}
		if !enabled {
			return nil, fmt.Errorf("tool %s is disabled", funcTool.ToolName())
		}
	}
	result, err := funcTool.OnInvokeTool(ctx, arguments)

	if err != nil {
		if funcTool.FailureErrorFunction != nil {
			errorFunc := *funcTool.FailureErrorFunction
			val, _ := errorFunc(ctx, err)
			return val, nil
		}
		val, _ := DefaultToolErrorFunction(ctx, err)
		return val, nil
	}
	return result, nil
}

func inputToItems(input Input) []responses.ResponseInputItemUnionParam{
	switch v := input.(type){
	case InputString:
		return []responses.ResponseInputItemUnionParam{
			responses.ResponseInputItemParamOfMessage(
				string(v),
				responses.EasyInputMessageRole(responses.ResponseInputMessageItemRoleUser),),
		}
	case InputItems:
		return []responses.ResponseInputItemUnionParam(v)
	default:
		panic(fmt.Errorf("unexpected Input type %T", v))
	}
}

func toolsToParams(tools []Tool) []responses.ToolUnionParam{
	if len(tools) == 0{
		return nil		
	}

	params := make([]responses.ToolUnionParam, 0, len(tools))

	for _, tool := range tools{
		funcTool, ok:= tool.(FunctionTool)
		if !ok{
			continue
		}

		toolParam := responses.ToolUnionParam{
			OfFunction: &responses.FunctionToolParam{
				Name: funcTool.Name,
				Description: param.NewOpt(funcTool.Description),
				Parameters: funcTool.ParamsJSONSchema,
				Strict: funcTool.StrictJSONSchema,
			},
		}
		params = append(params, toolParam)
	}
	return params
}
